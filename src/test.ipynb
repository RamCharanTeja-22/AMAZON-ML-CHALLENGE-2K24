{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b18825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE  # Import SMOTE instead of ADASYN\n",
    "\n",
    "def train_model(train_data):\n",
    "    # Vectorize the 'entity_name' feature\n",
    "    vectorizer = TfidfVectorizer(dtype=np.float32, max_features=500)\n",
    "    X = vectorizer.fit_transform(train_data['entity_name'])\n",
    "    y = train_data['group_id']\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = y.value_counts()\n",
    "    print(f\"Class distribution: {class_counts}\")\n",
    "    \n",
    "    # Filter out classes with very few samples\n",
    "    min_samples_per_class = 10\n",
    "    valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
    "    X_filtered = X[y.isin(valid_classes)]\n",
    "    y_filtered = y[y.isin(valid_classes)]\n",
    "    \n",
    "    # Apply SMOTE instead of ADASYN\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_filtered, y_filtered)\n",
    "        print(\"Applied SMOTE resampling.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Resampling failed: {e}. Proceeding without resampling.\")\n",
    "        X_resampled, y_resampled = X_filtered, y_filtered\n",
    "    \n",
    "    # Limit the size of the data to manage memory usage\n",
    "    max_samples = 5000\n",
    "    if X_resampled.shape[0] > max_samples:\n",
    "        X_resampled, _, y_resampled, _ = train_test_split(X_resampled, y_resampled, train_size=max_samples, random_state=42)\n",
    "    \n",
    "    # Split the data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Classifiers to try\n",
    "    classifiers = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "        'LogisticRegression': LogisticRegression(solver='liblinear', max_iter=100),\n",
    "        'SVC': SVC(kernel='linear', probability=True, random_state=42)\n",
    "    }\n",
    "    \n",
    "    best_clf = None\n",
    "    best_accuracy = 0\n",
    "    best_f1_score = 0\n",
    "    best_clf_name = None\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():\n",
    "        print(f\"Training with {clf_name}...\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        val_predictions = clf.predict(X_val)\n",
    "        val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "        val_f1_score = f1_score(y_val, val_predictions, average='macro')\n",
    "        print(f\"{clf_name} - Validation Accuracy: {val_accuracy:.2f}, F1 Score: {val_f1_score:.2f}\")\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_f1_score = val_f1_score\n",
    "            best_clf = clf\n",
    "            best_clf_name = clf_name\n",
    "    \n",
    "    print(f\"Best Classifier: {best_clf_name} with Accuracy: {best_accuracy:.2f} and F1 Score: {best_f1_score:.2f}\")\n",
    "    return best_clf, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb20c14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifiers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and evaluate each classifier\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_name, clf \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclassifiers\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Check if classifier is XGBoost (which requires dense input)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifiers' is not defined"
     ]
    }
   ],
   "source": [
    "# Train and evaluate each classifier\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Training with {clf_name}...\")\n",
    "    \n",
    "    # Check if classifier is XGBoost (which requires dense input)\n",
    "    if clf_name == 'XGBoost':\n",
    "        X_train_dense = X_train.toarray()  # Convert to dense format for XGBoost\n",
    "        X_val_dense = X_val.toarray()      # Convert to dense format for XGBoost\n",
    "        clf.fit(X_train_dense, y_train)\n",
    "        val_predictions = clf.predict(X_val_dense)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "        val_predictions = clf.predict(X_val)\n",
    "        \n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    val_f1_score = f1_score(y_val, val_predictions, average='macro')\n",
    "    print(f\"{clf_name} - Validation Accuracy: {val_accuracy:.2f}, F1 Score: {val_f1_score:.2f}\")\n",
    "    \n",
    "    # Update best classifier based on accuracy\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_f1_score = val_f1_score\n",
    "        best_clf = clf\n",
    "        best_clf_name = clf_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba2c11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: group_id\n",
      "459516    2678\n",
      "281678    1871\n",
      "308856    1738\n",
      "993359    1602\n",
      "731432    1545\n",
      "          ... \n",
      "362818       1\n",
      "494658       1\n",
      "679049       1\n",
      "522504       1\n",
      "220662       1\n",
      "Name: count, Length: 235, dtype: int64\n",
      "Filtered class distribution:\n",
      "group_id\n",
      "459516    2678\n",
      "281678    1871\n",
      "308856    1738\n",
      "993359    1602\n",
      "731432    1545\n",
      "          ... \n",
      "737148      13\n",
      "306956      11\n",
      "594224      11\n",
      "152339      11\n",
      "686198      10\n",
      "Name: count, Length: 201, dtype: int64\n",
      "Class distribution is balanced, skipping resampling.\n",
      "Training with RandomForest...\n",
      "RandomForest - Validation Accuracy: 0.11, F1 Score: 0.01\n",
      "Training with XGBoost...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195], got [107694 120219 120569 124643 130591 132401 140266 145452 149159 150535\n 150913 152057 152339 156839 171418 178778 178958 179080 180410 181357\n 186035 192132 204245 208023 211213 221399 225091 226504 237000 241608\n 245959 249638 252585 252782 254046 254449 267182 267482 268003 271537\n 273748 275506 276700 281678 288715 296366 296953 297918 299791 306956\n 308671 308856 311997 318770 329793 334327 347404 348551 355666 359859\n 365637 369753 373285 375816 386873 396159 397856 407921 411423 412008\n 416664 418636 426261 433914 442321 446789 449021 449805 453674 459271\n 459516 462757 469317 479564 483370 486636 487566 488883 489118 501250\n 507467 507619 507848 507988 518578 519155 522832 523149 524117 524635\n 529606 549052 550840 557758 558374 558832 563130 564709 569206 569657\n 589105 593600 599772 601746 609802 611510 625310 625842 628971 630390\n 630869 639090 639475 639508 641642 648011 654649 658003 666046 675317\n 681445 686198 701880 704724 709627 730429 731252 731432 737148 746096\n 748919 749917 750220 751532 752266 767202 776058 788365 794161 801829\n 801837 802198 804621 810266 825239 844474 847223 858439 860821 866516\n 866950 872083 881883 884560 885644 892291 893692 898898 907907 908443\n 916768 917343 918474 922709 926285 928606 929999 932012 933453 934747\n 939129 939587 952353 953031 955292 957050 957185 965518 969033 978900\n 983323 986984 991868 993359 997176 998545]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 147\u001b[0m\n\u001b[0;32m    133\u001b[0m constants \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgram\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_height\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcentimetre\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_volume\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcentilitre\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcubic foot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcubic inch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecilitre\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfluid ounce\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgallon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimperial gallon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlitre\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrolitre\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmillilitre\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquart\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m    144\u001b[0m }\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m best_clf, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Predict on the test dataset and save the output\u001b[39;00m\n\u001b[0;32m    150\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASET_FOLDER, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_out.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 80\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m     78\u001b[0m     X_train_dense \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mtoarray()  \u001b[38;5;66;03m# Convert sparse matrix to dense\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     X_val_dense \u001b[38;5;241m=\u001b[39m X_val\u001b[38;5;241m.\u001b[39mtoarray()      \u001b[38;5;66;03m# Convert validation set to dense\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_dense\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     val_predictions \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_val_dense)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1491\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1486\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1488\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1494\u001b[0m     )\n\u001b[0;32m   1496\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195], got [107694 120219 120569 124643 130591 132401 140266 145452 149159 150535\n 150913 152057 152339 156839 171418 178778 178958 179080 180410 181357\n 186035 192132 204245 208023 211213 221399 225091 226504 237000 241608\n 245959 249638 252585 252782 254046 254449 267182 267482 268003 271537\n 273748 275506 276700 281678 288715 296366 296953 297918 299791 306956\n 308671 308856 311997 318770 329793 334327 347404 348551 355666 359859\n 365637 369753 373285 375816 386873 396159 397856 407921 411423 412008\n 416664 418636 426261 433914 442321 446789 449021 449805 453674 459271\n 459516 462757 469317 479564 483370 486636 487566 488883 489118 501250\n 507467 507619 507848 507988 518578 519155 522832 523149 524117 524635\n 529606 549052 550840 557758 558374 558832 563130 564709 569206 569657\n 589105 593600 599772 601746 609802 611510 625310 625842 628971 630390\n 630869 639090 639475 639508 641642 648011 654649 658003 666046 675317\n 681445 686198 701880 704724 709627 730429 731252 731432 737148 746096\n 748919 749917 750220 751532 752266 767202 776058 788365 794161 801829\n 801837 802198 804621 810266 825239 844474 847223 858439 860821 866516\n 866950 872083 881883 884560 885644 892291 893692 898898 907907 908443\n 916768 917343 918474 922709 926285 928606 929999 932012 933453 934747\n 939129 939587 952353 953031 955292 957050 957185 965518 969033 978900\n 983323 986984 991868 993359 997176 998545]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to train the model using different classifiers\n",
    "def train_model(train_data):\n",
    "    # Vectorize the 'entity_name' feature\n",
    "    vectorizer = TfidfVectorizer(dtype=np.float32, max_features=500)\n",
    "    X = vectorizer.fit_transform(train_data['entity_name'])\n",
    "    y = train_data['group_id']\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = y.value_counts()\n",
    "    print(f\"Class distribution: {class_counts}\")\n",
    "    \n",
    "    # Filter out classes with very few samples\n",
    "    min_samples_per_class = 10\n",
    "    valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
    "    X_filtered = X[y.isin(valid_classes)]\n",
    "    y_filtered = y[y.isin(valid_classes)]\n",
    "    \n",
    "    # Check the filtered class distribution\n",
    "    filtered_class_counts = y_filtered.value_counts()\n",
    "    print(f\"Filtered class distribution:\\n{filtered_class_counts}\")\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if len(filtered_class_counts) > 1 and filtered_class_counts.min() >= min_samples_per_class:\n",
    "        print(\"Class distribution is balanced, skipping resampling.\")\n",
    "        X_resampled, y_resampled = X_filtered, y_filtered\n",
    "    else:\n",
    "        # Use ADASYN for resampling (with sampling_strategy parameter adjustment)\n",
    "        try:\n",
    "            adasyn = ADASYN(sampling_strategy=0.5, random_state=42)\n",
    "            X_resampled, y_resampled = adasyn.fit_resample(X_filtered, y_filtered)\n",
    "            print(\"Applied ADASYN resampling.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"ADASYN failed: {e}\")\n",
    "            print(\"Switching to SMOTE for resampling.\")\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_filtered, y_filtered)\n",
    "    \n",
    "    # Limit the size of the data to manage memory usage\n",
    "    max_samples = 5000  # Further reduced number of samples\n",
    "    if X_resampled.shape[0] > max_samples:\n",
    "        X_resampled, _, y_resampled, _ = train_test_split(X_resampled, y_resampled, train_size=max_samples, random_state=42)\n",
    "    \n",
    "    # Split the data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Classifiers to try\n",
    "    classifiers = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "        'LogisticRegression': LogisticRegression(solver='liblinear', max_iter=100),\n",
    "        'SVC': SVC(kernel='linear', probability=True, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Track the best model\n",
    "    best_clf = None\n",
    "    best_accuracy = 0\n",
    "    best_f1_score = 0\n",
    "    best_clf_name = None\n",
    "    \n",
    "    # Train and evaluate each classifier\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        print(f\"Training with {clf_name}...\")\n",
    "\n",
    "        # Convert to dense matrix if classifier is XGBoost\n",
    "        if clf_name == 'XGBoost':\n",
    "            X_train_dense = X_train.toarray()  # Convert sparse matrix to dense\n",
    "            X_val_dense = X_val.toarray()      # Convert validation set to dense\n",
    "            clf.fit(X_train_dense, y_train)\n",
    "            val_predictions = clf.predict(X_val_dense)\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)  # Other classifiers work with sparse data\n",
    "            val_predictions = clf.predict(X_val)\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "        val_f1_score = f1_score(y_val, val_predictions, average='macro')\n",
    "        print(f\"{clf_name} - Validation Accuracy: {val_accuracy:.2f}, F1 Score: {val_f1_score:.2f}\")\n",
    "        \n",
    "        # Update best classifier based on accuracy\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_f1_score = val_f1_score\n",
    "            best_clf = clf\n",
    "            best_clf_name = clf_name\n",
    "    \n",
    "    print(f\"Best Classifier: {best_clf_name} with Accuracy: {best_accuracy:.2f} and F1 Score: {best_f1_score:.2f}\")\n",
    "    return best_clf, vectorizer\n",
    "\n",
    "# Function to make predictions on the test data and save them in the required format\n",
    "def predict_and_save_output(clf, vectorizer, test_data, output_path, constants):\n",
    "    # Vectorize the 'entity_name' feature from test data\n",
    "    X_test = vectorizer.transform(test_data['entity_name'])\n",
    "    \n",
    "    # Convert to dense format if XGBoost classifier\n",
    "    if isinstance(clf, XGBClassifier):\n",
    "        X_test = X_test.toarray()\n",
    "\n",
    "    # Predict the group_id for the test data\n",
    "    test_predictions = clf.predict(X_test)\n",
    "    \n",
    "    # Create predictions in the required format\n",
    "    output = []\n",
    "    for index, pred in zip(test_data['index'], test_predictions):\n",
    "        # Map the group_id back to an entity name and format the entity value with units\n",
    "        entity_name = test_data.loc[test_data['index'] == index, 'entity_name'].values[0]\n",
    "        # Handle the units for predictions\n",
    "        unit = constants.get(entity_name, 'unit')\n",
    "        prediction = f\"{pred} {unit}\"\n",
    "        output.append([index, prediction])\n",
    "    \n",
    "    # Convert the list to a DataFrame and save it to CSV\n",
    "    output_df = pd.DataFrame(output, columns=['index', 'prediction'])\n",
    "    output_df.to_csv(output_path, index=False)\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "# Load the dataset\n",
    "DATASET_FOLDER = r'C:\\Users\\DELL\\OneDrive\\Desktop\\Amazon_ml_2024\\Amazon ML\\Amazon-ML-Challenge-2024\\dataset'\n",
    "train_data = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "test_data = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
    "\n",
    "# Load allowed units and constants\n",
    "constants = {\n",
    "    \"item_weight\": \"gram\",\n",
    "    \"item_height\": \"centimetre\",\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "best_clf, vectorizer = train_model(train_data)\n",
    "\n",
    "# Predict on the test dataset and save the output\n",
    "output_path = os.path.join(DATASET_FOLDER, 'test_out.csv')\n",
    "predict_and_save_output(best_clf, vectorizer, test_data, output_path, constants)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
